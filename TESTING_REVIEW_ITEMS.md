# Testing Matrix - Human Review Items
**Date:** 2025-10-11
**Purpose:** Items requiring human judgment and strategic decision-making

This document contains testing decisions that require human review due to:
- Strategic trade-offs (time vs coverage)
- Design decisions (architecture impact)
- Resource constraints (CI/CD time, maintenance burden)
- Risk assessment (what's acceptable vs critical)

---

## üìã Review Item 1: Integration Test Priorities

### Context
After analysis, the codebase has **strong unit test coverage** (~545 tests) but **gaps in integration tests** for cross-cutting scenarios.

### Decision Needed
**Which integration tests should be prioritized given limited time/resources?**

### Options

#### Option A: Critical Path First (Recommended)
**Focus:** Test what users actually do most often

**Priority 1: Installation workflow** (vm-installer)
- Risk: Installation failures block all usage
- Impact: High - affects all new users
- Estimated effort: 2-3 days
- Tests to add:
  - Full install from source
  - PATH updates across shells (bash, zsh, fish)
  - Upgrade scenarios
  - Permission handling

**Priority 2: Plugin system** (vm-plugin)
- Risk: Broken plugins = broken presets
- Impact: Medium - affects advanced users
- Estimated effort: 1 day
- Tests to add:
  - Plugin loading from directories
  - Invalid plugin handling
  - Plugin priority/override
  - Malformed YAML handling

**Priority 3: Docker registry integration** (vm-docker-registry)
- Risk: Registry failures = slow builds
- Impact: Medium - has fallback (Docker Hub)
- Estimated effort: 2 days
- Tests to add:
  - Pull-through caching
  - Docker daemon integration
  - Garbage collection
  - Network failure recovery

#### Option B: Coverage Completion
**Focus:** Fill all gaps systematically

- Same tests as Option A, plus:
- vm-platform cross-platform tests
- vm-core file system tests
- Multi-VM complex scenarios
- Service dependency chains

Estimated effort: 5-6 days

#### Option C: High-Value Scenarios Only
**Focus:** Test complex workflows, trust unit tests for basics

**Areas:**
- Multi-VM workflows (create multiple VMs, test interactions)
- Service dependency chains (postgres ‚Üí redis ‚Üí app)
- Error recovery (network failures, Docker crashes)
- Concurrent operations (multiple developers, shared services)

Estimated effort: 3-4 days

### My Recommendation: **Option A (Critical Path First)**

**Reasoning:**
1. Installation is the first impression - must work flawlessly
2. Unit tests cover most basic functionality already
3. Plugin system is core to user experience (presets)
4. Docker registry is "nice to have" optimization
5. Can iterate and add more tests later

**Trade-offs:**
- ‚úÖ Pro: Focuses on user-facing issues
- ‚úÖ Pro: Delivers value incrementally
- ‚ö†Ô∏è  Con: Doesn't achieve "complete coverage"
- ‚ö†Ô∏è  Con: Some edge cases remain untested

### Questions for Human
1. What's the user pain point ranking? (Installation vs plugins vs performance)
2. What's the CI/CD time budget? (More tests = longer CI)
3. Is 85% coverage acceptable, or is 95% required?
4. How much time can be allocated to this work?

---

## üìã Review Item 2: Test Execution Time vs Coverage

### Context
Adding comprehensive integration tests will increase CI/CD time significantly.

**Current situation:**
- Unit tests: Fast (~30 seconds total)
- Integration tests: Moderate (~2-3 minutes with Docker)
- E2E tests: Slow (5-10 minutes per full workflow)

### Decision Needed
**What's the acceptable trade-off between test coverage and CI/CD speed?**

### Options

#### Option A: Fast CI, Comprehensive Nightly
**CI (on every push):**
- Unit tests only (~30s)
- Smoke tests (critical paths only, ~1-2 min)
- Total: <3 minutes

**Nightly (scheduled):**
- All integration tests
- All E2E tests
- Cross-platform matrix
- Total: ~30-45 minutes

**Trade-offs:**
- ‚úÖ Pro: Fast feedback loop
- ‚úÖ Pro: Doesn't block development
- ‚ö†Ô∏è  Con: Integration issues found later
- ‚ö†Ô∏è  Con: Possible main branch breakage

#### Option B: Moderate CI, No Nightly (Recommended)
**CI (on every push):**
- Unit tests (~30s)
- Integration tests (~5min)
- Critical E2E tests (~5min)
- Total: ~10 minutes

**No nightly needed**

**Trade-offs:**
- ‚úÖ Pro: Catches issues early
- ‚úÖ Pro: Simpler CI/CD setup
- ‚ö†Ô∏è  Con: Slower feedback (10 vs 3 minutes)
- ‚úÖ Pro: Main branch always healthy

#### Option C: Comprehensive CI
**CI (on every push):**
- Everything: unit, integration, E2E, cross-platform
- Total: ~20-30 minutes

**Trade-offs:**
- ‚úÖ Pro: Maximum confidence
- ‚úÖ Pro: No surprises
- ‚ö†Ô∏è  Con: Slow feedback (developers wait)
- ‚ö†Ô∏è  Con: High CI cost (if using paid runners)

### My Recommendation: **Option B (Moderate CI)**

**Reasoning:**
1. 10 minutes is acceptable for most teams
2. Catches issues before merge
3. Simpler than managing nightly builds
4. Most integration tests are fast anyway

**Implementation:**
```yaml
# .github/workflows/test.yml
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions-rs/toolchain@v1

      # Fast unit tests
      - name: Unit tests
        run: cargo test --workspace --lib
        timeout-minutes: 2

      # Integration tests
      - name: Integration tests
        run: cargo test --workspace --test '*'
        timeout-minutes: 8

      # Docker-dependent tests (if Docker available)
      - name: Docker tests
        if: runner.os == 'Linux'
        run: cargo test --workspace -- --ignored
        timeout-minutes: 5
```

### Questions for Human
1. What's the team's tolerance for CI wait time?
2. Are CI runners paid or free? (affects cost consideration)
3. How often do developers push? (affects productivity impact)
4. Is there a preference for fast feedback vs complete testing?

---

## üìã Review Item 3: Test Organization Strategy

### Context
Current test organization mixes concerns:
- `vm/tests/` has 11 files with varying purposes
- Some test VM operations, some test CLI, some test services
- Unclear naming: "service_lifecycle" means different things in different files

### Decision Needed
**Should we reorganize tests for clarity, or keep current structure?**

### Current Structure
```
vm/tests/
‚îú‚îÄ‚îÄ workflow_tests.rs          # Config CLI
‚îú‚îÄ‚îÄ config_cli_tests.rs        # Config CLI (different aspects)
‚îú‚îÄ‚îÄ pkg_cli_tests.rs           # Package CLI
‚îú‚îÄ‚îÄ service_lifecycle_integration_tests.rs  # Service management
‚îú‚îÄ‚îÄ port_forwarding_tests.rs   # Port forwarding
‚îú‚îÄ‚îÄ temp_workflow_tests.rs     # Temp VMs
‚îú‚îÄ‚îÄ ssh_refresh.rs             # SSH features
‚îú‚îÄ‚îÄ vm_ops/
‚îÇ   ‚îú‚îÄ‚îÄ create_destroy_tests.rs
‚îÇ   ‚îú‚îÄ‚îÄ interaction_tests.rs
‚îÇ   ‚îú‚îÄ‚îÄ service_lifecycle_tests.rs  # VM lifecycle!
‚îÇ   ‚îú‚îÄ‚îÄ status_tests.rs
‚îÇ   ‚îú‚îÄ‚îÄ feature_tests.rs
‚îÇ   ‚îú‚îÄ‚îÄ lifecycle_integration_tests.rs
‚îÇ   ‚îú‚îÄ‚îÄ multi_instance_tests.rs
‚îÇ   ‚îî‚îÄ‚îÄ provider_parity_tests.rs
```

### Options

#### Option A: Keep Current Structure
**Reasoning:** "Don't reorganize working code"

**Trade-offs:**
- ‚úÖ Pro: Zero risk of breaking tests
- ‚úÖ Pro: No immediate work needed
- ‚ö†Ô∏è  Con: Confusing for new contributors
- ‚ö†Ô∏è  Con: "service_lifecycle" ambiguity
- ‚ö†Ô∏è  Con: No clear pattern for future tests

#### Option B: Reorganize by Feature (Recommended)
**Proposed structure:**
```
vm/tests/
‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îú‚îÄ‚îÄ config_commands.rs     # Merge workflow_tests + config_cli_tests
‚îÇ   ‚îú‚îÄ‚îÄ pkg_commands.rs        # pkg_cli_tests
‚îÇ   ‚îî‚îÄ‚îÄ vm_commands.rs         # High-level CLI
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ service_manager.rs     # Rename service_lifecycle_integration_tests
‚îÇ   ‚îî‚îÄ‚îÄ shared_services.rs     # Postgres, Redis, etc
‚îú‚îÄ‚îÄ vm_operations/
‚îÇ   ‚îú‚îÄ‚îÄ lifecycle.rs           # create, destroy, start, stop
‚îÇ   ‚îú‚îÄ‚îÄ interaction.rs         # ssh, exec, logs
‚îÇ   ‚îú‚îÄ‚îÄ status.rs              # status, list
‚îÇ   ‚îú‚îÄ‚îÄ features.rs            # Feature flags
‚îÇ   ‚îî‚îÄ‚îÄ multi_instance.rs      # Multi-instance
‚îú‚îÄ‚îÄ networking/
‚îÇ   ‚îú‚îÄ‚îÄ port_forwarding.rs
‚îÇ   ‚îî‚îÄ‚îÄ ssh_refresh.rs
‚îú‚îÄ‚îÄ temp_vms/
‚îÇ   ‚îî‚îÄ‚îÄ temp_workflow.rs
‚îî‚îÄ‚îÄ integration/
    ‚îú‚îÄ‚îÄ provider_parity.rs
    ‚îî‚îÄ‚îÄ full_lifecycle.rs
```

**Trade-offs:**
- ‚úÖ Pro: Clear organization
- ‚úÖ Pro: Easy to find tests
- ‚úÖ Pro: Clear pattern for future tests
- ‚ö†Ô∏è  Con: Requires refactoring work (~1-2 days)
- ‚ö†Ô∏è  Con: Risk of breaking tests during move

#### Option C: Hybrid - Only Rename Confusing Files
**Rename only:**
- `service_lifecycle_integration_tests.rs` ‚Üí `service_manager_integration_tests.rs`
- `vm_ops/service_lifecycle_tests.rs` ‚Üí `vm_ops/vm_lifecycle_tests.rs`

**Keep everything else as-is**

**Trade-offs:**
- ‚úÖ Pro: Minimal risk
- ‚úÖ Pro: Fixes main confusion
- ‚ö†Ô∏è  Con: Doesn't fully solve organization problem
- ‚úÖ Pro: Low effort (~15 minutes)

### My Recommendation: **Option C (Hybrid)**

**Reasoning:**
1. The main issue is naming ambiguity ("service_lifecycle")
2. Two simple renames fix 90% of the confusion
3. Reorganizing isn't urgent (tests work fine)
4. Low risk, high value

**Implementation:**
```bash
# Rename to clarify scope
cd vm/tests
git mv service_lifecycle_integration_tests.rs service_manager_integration_tests.rs
git mv vm_ops/service_lifecycle_tests.rs vm_ops/vm_lifecycle_tests.rs

# Update internal references
sed -i 's/service_lifecycle_tests/vm_lifecycle_tests/g' vm_ops.rs
```

### Questions for Human
1. Is the current organization causing real problems?
2. Are new contributors getting confused?
3. Is there a style guide for test organization?
4. Worth spending 1-2 days on full reorganization?

---

## üìã Review Item 4: Coverage Metrics and Goals

### Context
Current workspace has ~545 tests with good coverage, but no formal coverage targets.

### Decision Needed
**What are the coverage goals and how should they be measured?**

### Current State (Estimated)
Based on test counts and package sizes:
- vm-config: ~85% (excellent)
- vm-package-server: ~90% (excellent)
- vm: ~80% (good)
- vm-provider: ~75% (good)
- vm-auth-proxy: ~70% (good, but focused on critical paths)
- vm-installer: ~60% (unit tests only)
- vm-docker-registry: ~65% (unit tests only)
- vm-plugin: ~70% (unit tests only)
- vm-core: ~60% (gaps in file system, command stream)
- vm-platform: ~55% (gaps in cross-platform behavior)
- **Workspace average: ~72%**

### Options

#### Option A: Package-Specific Targets
**Different targets per package based on criticality:**

| Package | Target | Rationale |
|---------|--------|-----------|
| vm-auth-proxy | 90% | Security-critical |
| vm-package-server | 85% | Data integrity |
| vm-provider | 85% | Core functionality |
| vm-config | 80% | Complex logic |
| vm | 80% | User-facing |
| vm-installer | 80% | First impression |
| vm-docker-registry | 70% | Optional feature |
| vm-plugin | 70% | Simple logic |
| vm-core | 75% | Foundation |
| vm-platform | 65% | Platform-specific |
| Others | 60% | Utilities |

**Trade-offs:**
- ‚úÖ Pro: Realistic and achievable
- ‚úÖ Pro: Focuses effort where it matters
- ‚ö†Ô∏è  Con: More complex to track
- ‚úÖ Pro: Allows pragmatic decisions

#### Option B: Uniform 80% Target
**All packages must have 80% coverage**

**Trade-offs:**
- ‚úÖ Pro: Simple to communicate
- ‚úÖ Pro: Easy to measure
- ‚ö†Ô∏è  Con: May be overkill for simple packages
- ‚ö†Ô∏è  Con: May be insufficient for critical packages
- ‚ö†Ô∏è  Con: "Gaming" possible (trivial tests to hit number)

#### Option C: Trend-Based (No Hard Targets) (Recommended)
**Focus on improving coverage over time, not hitting specific numbers**

**Rules:**
1. PRs cannot decrease coverage
2. New code must have tests
3. Bug fixes must include regression tests
4. No minimum percentage required

**Trade-offs:**
- ‚úÖ Pro: Prevents "gaming" metrics
- ‚úÖ Pro: Focus on quality, not quantity
- ‚úÖ Pro: Flexible and pragmatic
- ‚ö†Ô∏è  Con: Less concrete goal
- ‚ö†Ô∏è  Con: Harder to track progress

### My Recommendation: **Option C (Trend-Based)**

**Reasoning:**
1. Coverage percentage can be misleading (100% coverage ‚â† bug-free)
2. Quality of tests matters more than quantity
3. Current coverage (~72%) is already reasonable
4. Trend-based prevents coverage regression
5. Allows pragmatic decisions (skip trivial code)

**Implementation:**
```yaml
# .github/workflows/coverage.yml
- name: Check coverage trend
  run: |
    NEW_COV=$(cargo tarpaulin --workspace --out Xml | grep line-rate)
    BASE_COV=$(git show origin/main:.coverage | grep line-rate)
    if (( $(echo "$NEW_COV < $BASE_COV" | bc -l) )); then
      echo "Coverage decreased from $BASE_COV to $NEW_COV"
      exit 1
    fi
```

### Questions for Human
1. Is there an existing coverage policy?
2. What's more important: high coverage or quality tests?
3. Are there compliance requirements (e.g., 80% mandated)?
4. Should we track coverage at all, or focus on test quality?

---

## üìã Review Item 5: Test Infrastructure Investment

### Context
Currently, each test file duplicates setup code (fixtures, helpers, assertions).

### Decision Needed
**Should we invest in shared test infrastructure (test-utils crate)?**

### Current Duplication Examples

**Fixture creation** (repeated in ~8 files):
```rust
struct WorkflowTestFixture {
    _temp_dir: TempDir,
    test_dir: PathBuf,
    binary_path: PathBuf,
}

impl WorkflowTestFixture {
    fn new() -> Result<Self> {
        let temp_dir = TempDir::new()?;
        let test_dir = temp_dir.path().join("test_project");
        fs::create_dir_all(&test_dir)?;
        // ... find binary path ...
        Ok(Self { _temp_dir, test_dir, binary_path })
    }
}
```

**Container cleanup** (repeated in ~5 files):
```rust
fn cleanup_test_containers(&self) -> Result<()> {
    let _ = Command::new("docker")
        .args(["rm", "-f", &self.project_name])
        .output();
    Ok(())
}
```

**Assertions** (custom patterns in multiple files):
```rust
assert!(
    output.status.success(),
    "Command failed: {}",
    String::from_utf8_lossy(&output.stderr)
);
```

### Options

#### Option A: Create test-utils Crate (Recommended)
**Create:** `rust/test-utils/`

**Contents:**
```rust
// test-utils/src/lib.rs
pub mod fixtures;     // Shared test fixtures
pub mod assertions;   // Custom assertions
pub mod docker;       // Docker test helpers
pub mod temp;         // Temp directory helpers
pub mod binary;       // Binary path resolution

// Examples:
pub use fixtures::VmTestFixture;
pub use docker::cleanup_containers;
pub use assertions::assert_success;
```

**Benefits:**
- ‚úÖ Reduce code duplication (~500 LOC saved)
- ‚úÖ Consistent test patterns
- ‚úÖ Easier to write new tests
- ‚úÖ Single place to fix bugs

**Costs:**
- ‚ö†Ô∏è  Initial setup: ~1 day
- ‚ö†Ô∏è  Migration: ~2 days
- ‚ö†Ô∏è  Maintenance: minimal

**Trade-offs:**
- ‚úÖ Pro: DRY principle
- ‚úÖ Pro: Better test quality
- ‚ö†Ô∏è  Con: Another dependency
- ‚ö†Ô∏è  Con: Migration effort

#### Option B: Shared Helpers in tests/common/
**Create:** `vm/tests/common/mod.rs` with shared helpers

**Benefits:**
- ‚úÖ Simpler than full crate
- ‚úÖ Still reduces duplication
- ‚ö†Ô∏è  Only available within vm package

**Costs:**
- ‚ö†Ô∏è  Setup: ~4 hours
- ‚ö†Ô∏è  Migration: ~1 day

#### Option C: Keep Current Duplication
**Do nothing**

**Trade-offs:**
- ‚úÖ Pro: Zero effort
- ‚ö†Ô∏è  Con: Continued duplication
- ‚ö†Ô∏è  Con: Harder to maintain consistency
- ‚ö†Ô∏è  Con: More code to review in PRs

### My Recommendation: **Option A (test-utils crate)**

**Reasoning:**
1. ~500 LOC duplication is significant
2. Multiple packages need same helpers
3. Investment pays off quickly (2-3 days work, saves hours on future tests)
4. Improves test quality through consistency
5. Standard pattern in Rust projects

**Phased Implementation:**
```markdown
Phase 1 (Day 1): Create test-utils crate
- Set up Cargo.toml
- Add basic fixtures module
- Add docker helpers module
- Add assertion module

Phase 2 (Day 2): Migrate vm tests
- Migrate vm/tests to use test-utils
- Verify all tests still pass

Phase 3 (Day 3): Migrate other packages
- Migrate vm-config tests
- Migrate vm-provider tests
- Migrate vm-package-server tests

Phase 4 (Optional): Advanced features
- Add contract test framework
- Add property-based testing helpers
- Add benchmark helpers
```

### Questions for Human
1. Is code duplication causing maintenance issues?
2. Is 3 days of work acceptable for this improvement?
3. Are there existing patterns/libraries preferred?
4. Should this be done now or deferred?

---

## üìã Review Item 6: Cross-Platform Testing Strategy

### Context
The project supports Linux, macOS, and Windows, but most tests only run on Linux.

**Current situation:**
- Primary CI: Linux only
- tart_provider_tests: macOS only (correctly)
- No Windows-specific tests
- No cross-platform parity tests

### Decision Needed
**How much should we invest in cross-platform testing?**

### Options

#### Option A: Linux Primary, Others Secondary
**CI Matrix:**
- Linux: All tests (required to pass)
- macOS: All tests (required to pass)
- Windows: All tests (optional/allowed to fail)

**Rationale:** Most users on Linux/macOS, Windows is experimental

**Trade-offs:**
- ‚úÖ Pro: Pragmatic (matches user base)
- ‚úÖ Pro: Doesn't block development
- ‚ö†Ô∏è  Con: Windows users may hit bugs
- ‚úÖ Pro: Lower CI cost

#### Option B: Full Parity (Recommended)
**CI Matrix:**
- Linux: All tests (required)
- macOS: All tests (required)
- Windows: All tests (required)

**Rationale:** Professional quality, no platform is second-class

**Trade-offs:**
- ‚úÖ Pro: Equal quality on all platforms
- ‚úÖ Pro: Catches platform-specific bugs
- ‚ö†Ô∏è  Con: Higher CI cost (~3x)
- ‚ö†Ô∏è  Con: More flaky tests to handle
- ‚ö†Ô∏è  Con: Slower feedback (wait for all platforms)

#### Option C: Staged Rollout
**Phase 1 (Now):**
- Linux: All tests (required)
- macOS: All tests (optional)
- Windows: Smoke tests only (optional)

**Phase 2 (After stabilization):**
- All platforms: All tests (required)

**Trade-offs:**
- ‚úÖ Pro: Incremental approach
- ‚úÖ Pro: Stabilize tests on Linux first
- ‚ö†Ô∏è  Con: Temporary quality difference

### My Recommendation: **Option B (Full Parity)**

**Reasoning:**
1. Modern CI (GitHub Actions) makes this easy and free for OSS
2. Platform-specific bugs are hard to debug if found late
3. Sets high quality bar from start
4. Many Rust projects do this successfully

**Implementation:**
```yaml
# .github/workflows/test.yml
strategy:
  matrix:
    os: [ubuntu-latest, macos-latest, windows-latest]
    rust: [stable]
  fail-fast: false  # Don't cancel other jobs if one fails

runs-on: ${{ matrix.os }}
```

**Caveats:**
- Some tests may need platform-specific adjustments
- Docker availability varies by platform
- May need to mark some tests as platform-specific

### Questions for Human
1. What's the target user platform distribution?
2. Is Windows support a priority or nice-to-have?
3. Is there a CI cost constraint?
4. Acceptable to have slower CI for better quality?

---

## üìã Summary & Decision Matrix

| Item | Recommendation | Confidence | Effort | Risk |
|------|---------------|------------|--------|------|
| 1. Test priorities | Option A: Critical path | 85% | 2-3 days | Low |
| 2. CI execution time | Option B: Moderate CI | 90% | 2 hours setup | Low |
| 3. Test organization | Option C: Hybrid rename | 95% | 15 minutes | Very low |
| 4. Coverage goals | Option C: Trend-based | 80% | 2 hours setup | Low |
| 5. Test infrastructure | Option A: test-utils crate | 85% | 3 days | Low |
| 6. Cross-platform | Option B: Full parity | 90% | 4 hours setup | Medium |

**Total Estimated Effort:** ~5-6 days for all recommendations

**Recommended Priority Order:**
1. **Item 3** (15 min) - Quick win, fixes confusion
2. **Item 2** (2 hours) - CI setup, enables other work
3. **Item 6** (4 hours) - Cross-platform CI, catches bugs early
4. **Item 5** (3 days) - Test infrastructure, improves developer experience
5. **Item 1** (2-3 days) - Integration tests, fills gaps
6. **Item 4** (2 hours) - Coverage tracking, optional but good practice

**Can be done in parallel:**
- Items 2, 3, 6 (CI/organization setup)
- Items 1, 5 (testing work)

---

## üéØ Next Steps

1. **Human Review:** Review these recommendations and provide feedback
2. **Decision:** Choose options for each item (or accept recommendations)
3. **Prioritize:** Decide which items to tackle first
4. **Assign:** Allocate resources/time for implementation
5. **Execute:** Follow implementation plans in this document
6. **Track:** Monitor progress and adjust as needed

**Questions? Concerns? Alternative ideas?**
- Open for discussion on any recommendation
- Can provide more detail on any option
- Can analyze additional trade-offs if needed
